{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged successfully. Output saved as 'merged_cars_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# List of file names\n",
    "bath = \"Dataset\\\\\"\n",
    "file_names = ['bangalore_cars.xlsx', 'chennai_cars.xlsx', 'delhi_cars.xlsx',\n",
    "              'hyderabad_cars.xlsx', 'jaipur_cars.xlsx', 'kolkata_cars.xlsx']\n",
    "dfs = []\n",
    "\n",
    "for file in file_names:\n",
    "    df = pd.read_excel(f\"{bath}{file}\")\n",
    "    state = file.split('_')[0]\n",
    "    df['state'] = state\n",
    "   \n",
    "    dfs.append(df)\n",
    " \n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    " \n",
    "# merged_df.to_excel('merged_cars_data.xlsx', index=False)\n",
    " \n",
    "print(\"Files merged successfully. Output saved as 'merged_cars_data.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Extract the Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(row):\n",
    "    try:\n",
    "        a = ast.literal_eval(row)\n",
    "        return a\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def first(car_data):\n",
    "    df = {}\n",
    "    for row in car_data:\n",
    "        for key in row:\n",
    "            if key not in df:\n",
    "                df[key] = []\n",
    "            df[key].append(row[key])\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "def top(car_data):\n",
    "        def extract_key_value_pairs(d):\n",
    "            flat_dict = {}\n",
    "            try:\n",
    "                if 'top' in d:\n",
    "                    for item in d['top']:\n",
    "                        key = item.get('key')  # Extracting the key\n",
    "                        value = item.get('value') # Extracting the value\n",
    "                        if key:\n",
    "                            flat_dict[key] = value  # Use key as column name, value as entry\n",
    "            except:\n",
    "                pass\n",
    "            return flat_dict\n",
    "        flattened_data = [extract_key_value_pairs(d) for d in car_data if d is not None]\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        return df\n",
    "\n",
    "\n",
    "def data(car_data):\n",
    "    def extract_key_value_pairs(d):\n",
    "        flat_dict = {}\n",
    "        try:\n",
    "            # Proceed only if 'data' exists in the dictionary\n",
    "            if d is not None and 'data' in d:\n",
    "                for section in d['data']:\n",
    "                    if 'list' in section:\n",
    "                        for item in section['list']:\n",
    "                            key = item.get('key')  # Using .get to avoid KeyError\n",
    "                            value = item.get('value')\n",
    "                            if key and value:  # Ensure both key and value exist\n",
    "                                flat_dict[key] = value\n",
    "        except Exception as e:\n",
    "            # Log or handle exception if needed\n",
    "            print(f\"Error processing row: {e}\")\n",
    "        return flat_dict\n",
    "\n",
    "    # Filter out None entries\n",
    "    flattened_data = [extract_key_value_pairs(d) for d in car_data if d is not None]\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_columns = ['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs']\n",
    "\n",
    "for columns in car_columns:\n",
    "    merged_df[columns] = merged_df[columns].apply(convert_to_dict)\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "q = pd.DataFrame()\n",
    "for i in car_columns:\n",
    "    w = first(merged_df[i])\n",
    "\n",
    "    if w is None:\n",
    "        continue\n",
    "    columns = w.columns\n",
    "\n",
    "    if \"top\" in columns:\n",
    "        w = top(merged_df[i])\n",
    "\n",
    "    if \"data\" in columns:\n",
    "        q = data(merged_df[i])\n",
    "\n",
    "    final_df = pd.concat([final_df, w], axis=1, ignore_index=False)\n",
    "    if not q.empty:\n",
    "        final_df = pd.concat([final_df, q], axis=1, ignore_index=False)\n",
    "final_df['city'] = merged_df[\"state\"]\n",
    "final_df['car_links'] = merged_df['car_links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Remove Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 51\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a copy of the final dataframe\n",
    "df = final_df.copy()\n",
    "\n",
    "df_columns = df.columns\n",
    "for i,k in df.isna().sum().items():\n",
    "    if k >= 300:\n",
    "        df.drop(i, axis=1, inplace=True)\n",
    "df_columns_1 = df.columns\n",
    "print(len(df_columns),len(df_columns_1))\n",
    "\n",
    "df_1 = pd.DataFrame(df['Max Power'])\n",
    "df_1.columns = ['Max Power (bhp)', 'Max Power (bhp@rpm)']\n",
    "df = df.drop(columns=[\"Max Power\"])\n",
    "df['Max Power'] = df_1['Max Power (bhp)']\n",
    "\n",
    "\n",
    "conversion_factor = 1.39\n",
    "df['Mileage'] = df['Mileage'].str.replace(' kmpl', '') \n",
    "df['Mileage'] = df['Mileage'].apply(lambda x: float(x.replace(' km/kg', '')) * conversion_factor if 'km/kg' in str(x) else float(x))\n",
    "df['Mileage'] = df['Mileage'].astype(float)\n",
    "\n",
    "def extract_integer(value):\n",
    "    result = re.findall(r'\\d+', str(value))\n",
    "    return int(result[0]) if result else None\n",
    "\n",
    "\n",
    "df['km'] = df['km'].str.replace(',', '').astype(int)\n",
    "\n",
    "df = df.drop([\"priceActual\",\"priceSaving\",\"Year of Manufacture\",\"Seats\",\"Fuel Type\",\"trendingText\",\"owner\",\"Ownership\",\"Kms Driven\",\"transmission\",'Engine Displacement','Displacement','Max Torque',\"car_links\",'centralVariantId'], axis=1)\n",
    "\n",
    "df['Insurance Validity'] = df['Insurance Validity'].replace({\n",
    "    'Third Party insurance': 'Third Party',\n",
    "    '1': 'Not Available',\n",
    "    '2': 'Not Available',\n",
    "})\n",
    "\n",
    "df['Gear Box'] = df['Gear Box'].str.strip()\n",
    "df['Gear Box'] = df['Gear Box'].replace({\n",
    "    '5-Speed': '5 Speed', '5-Speed`': '5 Speed', '5-Speed ': '5 Speed',\n",
    "    '5 speed': '5 Speed', '5-Speed ': '5 Speed',\n",
    "    'Six Speed Manual': '6 Speed', 'Six Speed': '6 Speed', '6-Speed': '6 Speed',\n",
    "    '6-Speed IVT': '6 Speed IVT', 'Six Speed Automatic Transmission': '6 Speed',\n",
    "    '6Speed': '6 Speed', 'Six Speed  Gearbox': '6 Speed', '6-speed': '6 Speed',\n",
    "    'Five Speed': '5 Speed', 'Five Speed Manual': '5 Speed',\n",
    "    'Five Speed Manual Transmission': '5 Speed', 'Five Speed Manual Transmission Gearbox': '5 Speed',\n",
    "    '7-Speed': '7 Speed', '7-speed': '7 Speed',\n",
    "    'Seven Speed': '7 Speed', '7-speed PDK': '7 Speed',\n",
    "    '7G-DCT': '7 Speed DCT', '7-Speed DCT': '7 Speed DCT',\n",
    "    '8-Speed': '8 Speed', '8-speed': '8 Speed',\n",
    "    '8Speed': '8 Speed', '8-Speed DCT': '8 Speed',\n",
    "    '9-Speed': '9 Speed', '9-speed': '9 Speed',\n",
    "    '10-speed': '10 Speed',\n",
    "    'Fully Automatic': 'Automatic', 'Single-speed transmission': 'Single Speed',\n",
    "    'Single speed reduction gear': 'Single Speed', 'Single Speed': 'Single Speed',\n",
    "    'SPEEDSHIFT TCT 9G': '9 Speed', '9G-TRONIC': '9 Speed',\n",
    "    '9G TRONIC': '9 Speed', '9G-TRONIC automatic': '9 Speed',\n",
    "    'AGS': 'Automatic', 'IVT': 'IVT', 'E-CVT': 'CVT', 'eCVT': 'CVT',\n",
    "    'AMG SPEEDSHIFT DCT 8G': '8 Speed DCT', 'AMG 7-SPEED DCT': '7 Speed DCT',\n",
    "    '10 Speed': '10 Speed'\n",
    "})\n",
    "\n",
    "df['Engine'] = df['Engine'].str.replace(' CC', '').astype(float)\n",
    "\n",
    "df['Max Power'] = df['Max Power'].apply(extract_integer)\n",
    "\n",
    "df['Torque'] = df['Torque'].apply(extract_integer)\n",
    "\n",
    "df['Length'] = df['Length'].apply(extract_integer)\n",
    "\n",
    "df['Width'] = df['Width'].apply(extract_integer)\n",
    "\n",
    "df['Wheel Base'] = df['Wheel Base'].apply(extract_integer)\n",
    "\n",
    "df['Height'] = df['Height'].apply(extract_integer)\n",
    "\n",
    "df['Kerb Weight'] = df['Kerb Weight'].apply(extract_integer)\n",
    "df['Kerb Weight'] = df['Kerb Weight'].fillna(df['Kerb Weight'].mean())\n",
    "\n",
    "df['Registration Year'] = df['Registration Year'].apply(extract_integer)\n",
    "\n",
    "df['Tyre Type'] = df['Tyre Type'].str.strip()\n",
    "df['Tyre Type'] = df['Tyre Type'].replace({\n",
    "    'Tubeless, Radial': 'Tubeless Radial', 'Tubeless,Radial': 'Tubeless Radial', \n",
    "    'Radial, Tubless': 'Tubeless Radial', 'Radial, Tubeless': 'Tubeless Radial',\n",
    "    'Tubeless Radial Tyres': 'Tubeless Radial', 'Tubless, Radial': 'Tubeless Radial',\n",
    "    'Radial,Tubeless': 'Tubeless Radial', 'Tubeless,Radials': 'Tubeless Radial',\n",
    "    'Tubeless, Runflat': 'Tubeless Runflat', 'Tubeless. Runflat': 'Tubeless Runflat',\n",
    "    'Runflat Tyres': 'Runflat', 'Runflat Tyre': 'Runflat', 'Runflat,Radial': 'Runflat Radial',\n",
    "    'Run-Flat': 'Runflat', 'Radial with tube': 'Radial Tube',\n",
    "    'Radial Tubeless': 'Tubeless Radial', 'Tubeless Radials Tyre': 'Tubeless Radial',\n",
    "    'Tubeless ': 'Tubeless', 'Radial ': 'Radial', \n",
    "    'tubeless tyre': 'Tubeless', 'Tubeless Tyre': 'Tubeless',\n",
    "    'Tubeless Tyres': 'Tubeless', 'Radial Tyres': 'Radial',\n",
    "    'Tubeless Tyres, Radial': 'Tubeless Radial', 'Tubeless,Radial ': 'Tubeless Radial',\n",
    "    'Tubeless Tyres Mud Terrain': 'Tubeless Mud Terrain',\n",
    "    'Tubeless Tyres All Terrain': 'Tubeless All Terrain',\n",
    "    'Tubeless Radials': 'Tubeless Radial','Tubeless,Runflat': 'Tubeless Runflat',\n",
    "    'Tubless,Radial': 'Tubeless Radial', 'Radial Tube': 'Radial', 'Tubeless, Radials': 'Tubeless Radial'\n",
    "    \n",
    "})\n",
    "\n",
    "def clean_price(price):\n",
    "    price = price.replace(',', '').replace('₹', '').strip()\n",
    "    try:\n",
    "        if 'Lakh' in price:\n",
    "            price = float(price.replace('Lakh', '').strip()) * 100000\n",
    "        elif 'Crore' in price:\n",
    "            price = float(price.replace('Crore', '').strip()) * 10000000\n",
    "        else:\n",
    "            price = float(price)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return price\n",
    "df['price'] = df['price'].apply(clean_price)\n",
    "df['price'] = df['price'].astype(int)\n",
    "\n",
    "\n",
    "categorical_columns = [\n",
    " \"ft\",\"bt\",\"ownerNo\",\"oem\",\"model\",\"modelYear\",\n",
    " \"Registration Year\",\"Insurance Validity\",\"Transmission\",\n",
    " \"Engine\",\"Color\",\"Engine Type\",\"No of Cylinder\", \"Values per Cylinder\",\n",
    " \"Gear Box\",\"Seating Capacity\",\"Steering Type\",\"Front Brake Type\",\n",
    " \"Rear Brake Type\",\"Tyre Type\",\"No Door Numbers\",\"city\"]\n",
    "\n",
    "Numerical_columns = [\"it\",\"km\",\"Mileage\",\"Torque\",\"Length\",\"Width\",\"Height\",\"Wheel Base\",\"Kerb Weight\",\"Max Power\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "for col in Numerical_columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoding_columns = [\n",
    "    'ft', 'bt', 'oem', 'model', 'Insurance Validity', 'Transmission',\n",
    "    'Color', 'Engine Type', 'Gear Box', 'Steering Type', \n",
    "    'Front Brake Type', 'Rear Brake Type', 'Tyre Type', 'city','variantName'\n",
    "]\n",
    "\n",
    "label_encoding_columns = [\n",
    "    'Engine', 'No of Cylinder', 'Values per Cylinder', \n",
    "    'Seating Capacity', 'No Door Numbers', 'ownerNo', \n",
    "    'modelYear', 'Registration Year'\n",
    "]\n",
    "\n",
    "numerical_columns = [\n",
    "    \"it\",\"km\", \"Mileage\", \"Torque\", \"Length\", \"Width\", \"Height\",\n",
    "    \"Wheel Base\", \"Kerb Weight\", \"Max Power\"\n",
    "]\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with low variance (variance = 0): ['it']\n"
     ]
    }
   ],
   "source": [
    "numerical_column = X[numerical_columns]\n",
    "\n",
    "variance = numerical_column.var()\n",
    "\n",
    "low_variance_columns = variance[variance == 0].index.tolist()\n",
    "\n",
    "print(\"Columns with low variance (variance = 0):\", low_variance_columns)\n",
    "\n",
    "drop_columns.append(low_variance_columns[0])\n",
    "numerical_columns.remove(low_variance_columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation\n",
    "numeric_df = X[numerical_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  \n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: \n",
    "                colname = corr_matrix.columns[i]  \n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "numeric_df = X[numerical_columns]\n",
    "corr_features = correlation(numeric_df, 0.9)\n",
    "\n",
    "print(corr_features)\n",
    "for i in corr_features:\n",
    "    drop_columns.append(i)\n",
    "    numerical_columns.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation with price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = pd.DataFrame(X[numerical_columns])\n",
    "\n",
    "X_corr['price'] = y.values  \n",
    "\n",
    "correlation_with_price = X_corr[numerical_columns + ['price']].corr()['price']\n",
    "\n",
    "print(\"Correlation with price:\\n\", correlation_with_price)\n",
    "\n",
    "high_correlation_features = correlation_with_price[abs(correlation_with_price) < 0.1].index.tolist()\n",
    "print(\"Highly correlated features with price (|correlation| < 0.1):\", high_correlation_features)\n",
    "for i in high_correlation_features:\n",
    "    drop_columns.append(i)\n",
    "    numerical_columns.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "categorical_columns = label_encoding_columns + OneHotEncoding_columns\n",
    "\n",
    "chi_square_results = []\n",
    "\n",
    "for i, col1 in enumerate(categorical_columns):\n",
    "    for j, col2 in enumerate(categorical_columns):\n",
    "        if i >= j:  \n",
    "            continue\n",
    "        \n",
    "        contingency_table = pd.crosstab(X[col1], X[col2])\n",
    "\n",
    "        chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        chi_square_results.append((col1, col2, chi2_stat, p_value))\n",
    "\n",
    "chi_square_df = pd.DataFrame(chi_square_results, columns=['Feature 1', 'Feature 2', 'Chi-Square Statistic', 'p-value'])\n",
    "\n",
    "pivot_table = chi_square_df.pivot(index='Feature 1', columns='Feature 2', values='Chi-Square Statistic')\n",
    "\n",
    "pivot_table = pivot_table.fillna(pivot_table.T)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pivot_table, cmap=\"YlGnBu\", annot=False, linewidths=0.5)\n",
    "plt.title(\"Heatmap of Chi-Square Statistics Between Categorical Features\")\n",
    "plt.xticks(rotation=90)  \n",
    "plt.yticks(rotation=0)   \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "chi_square_threshold = 10000 \n",
    "\n",
    "highly_correlated = chi_square_df[chi_square_df['Chi-Square Statistic'] > chi_square_threshold]\n",
    "\n",
    "highly_correlated = highly_correlated.sort_values(by='Chi-Square Statistic', ascending=False)\n",
    "\n",
    "highly_correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns based on Chi Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ = [\"Engine Type\",\"model\",\"variantName\",\"Registration Year\",'Rear Brake Type',\"Front Brake Type\",'Tyre Type']\n",
    "for i in drop_:\n",
    "    drop_columns.append(i)\n",
    "    try:\n",
    "        label_encoding_columns.remove(i)\n",
    "    except:\n",
    "        OneHotEncoding_columns.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anova test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "for feature in label_encoding_columns:\n",
    "    groups = [y[X[feature] == category] for category in X[feature].unique()]\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "\n",
    "    anova_results.append((feature, f_stat))\n",
    "\n",
    "for feature in OneHotEncoding_columns:\n",
    "    groups = [y[X[feature] == category] for category in X[feature].unique()]\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "    \n",
    "    anova_results.append((feature, f_stat))\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results, columns=['Feature', 'F_statistic'])\n",
    "\n",
    "anova_df = anova_df.sort_values(by='F_statistic', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features = anova_df.head(11)\n",
    "last_features = anova_df[11:]\n",
    "print(top_5_features['Feature'])\n",
    "print(last_features['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in last_features['Feature']:\n",
    "    print(i)\n",
    "    drop_columns.append(i)\n",
    "    try:\n",
    "        label_encoding_columns.remove(i)\n",
    "    except:\n",
    "        OneHotEncoding_columns.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=drop_columns)\n",
    "for col in label_encoding_columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(X[col]) \n",
    "    X[col] = label_encoder.transform(X[col])\n",
    "    with open(f'Models\\\\label_encoder_{col}.pkl', 'wb') as file:\n",
    "        pickle.dump(label_encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "X_train_encoded = pd.DataFrame(onehot_encoder.fit_transform(X_train[OneHotEncoding_columns]))\n",
    "X_test_encoded = pd.DataFrame(onehot_encoder.transform(X_test[OneHotEncoding_columns]))\n",
    "\n",
    "X_train_encoded.columns = onehot_encoder.get_feature_names_out(OneHotEncoding_columns)\n",
    "X_test_encoded.columns = onehot_encoder.get_feature_names_out(OneHotEncoding_columns)\n",
    "\n",
    "X_train_encoded.index = X_train.index\n",
    "X_test_encoded.index = X_test.index\n",
    "\n",
    "X_train = X_train.drop(columns=OneHotEncoding_columns)\n",
    "X_test = X_test.drop(columns=OneHotEncoding_columns)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_encoded], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression CV Scores: [-4.32386482e+24 -1.69427835e+25 -5.27174262e+21 -5.12399293e+23\n",
      " -4.10257173e+24]\n",
      "Linear Regression Mean CV R² Score: -5.177378223685654e+24\n",
      "\n",
      "Random Forest CV Scores: [0.61466488 0.92403906 0.87953269 0.85847013 0.87259771]\n",
      "Random Forest Mean CV R² Score: 0.8298608948968041\n",
      "\n",
      "Gradient Boosting CV Scores: [0.5796606  0.92134317 0.89102619 0.88542972 0.84811279]\n",
      "Gradient Boosting Mean CV R² Score: 0.825114492976506\n",
      "\n",
      "Support Vector Regressor CV Scores: [-0.05631644 -0.08965832 -0.06148658 -0.0781905  -0.06245647]\n",
      "Support Vector Regressor Mean CV R² Score: -0.06962166335698376\n",
      "\n",
      "K-Nearest Neighbors CV Scores: [0.5388989  0.84888337 0.65791461 0.72563647 0.6755662 ]\n",
      "K-Nearest Neighbors Mean CV R² Score: 0.6893799103938031\n",
      "\n",
      "Decision Tree CV Scores: [0.50157702 0.87822218 0.75973181 0.84444299 0.76498566]\n",
      "Decision Tree Mean CV R² Score: 0.749791930349686\n",
      "\n",
      "XGBoost CV Scores: [0.5977093  0.92810094 0.90203726 0.79699004 0.8625896 ]\n",
      "XGBoost Mean CV R² Score: 0.8174854278564453\n",
      "\n",
      "Best model: Random Forest with mean R² score of 0.8298608948968041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "svr_model = SVR()\n",
    "knn_model = KNeighborsRegressor()\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, eval_metric='rmse')\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Gradient Boosting': gb_model,\n",
    "    'Support Vector Regressor': svr_model,\n",
    "    'K-Nearest Neighbors': knn_model,\n",
    "    'Decision Tree': dt_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store cross-validation scores\n",
    "cv_scores = {}\n",
    "\n",
    "# Loop through each model, perform cross-validation, and store the results\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    cv_scores[model_name] = scores\n",
    "    print(f\"{model_name} CV Scores: {scores}\")\n",
    "    print(f\"{model_name} Mean CV R² Score: {np.mean(scores)}\\n\")\n",
    "\n",
    "# Find the model with the highest mean cross-validation R² score\n",
    "best_model = max(cv_scores, key=lambda x: np.mean(cv_scores[x]))\n",
    "best_score = np.mean(cv_scores[best_model])\n",
    "\n",
    "print(f\"Best model: {best_model} with mean R² score of {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_xgb = XGBRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [80, 100, 200, 300, 500],          # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],      # Learning rate\n",
    "    'max_depth': [3, 5, 7,10,50,100],                   # Maximum depth of a tree\n",
    "    'min_child_weight': [1, 3, 5],                # Minimum sum of instance weight(hessian) needed in a child\n",
    "    'gamma': [0, 0.1, 0.5],                  # Minimum loss reduction required to make a further partition\n",
    "    'subsample': [0.6, 0.8, 1.0],                 # Subsample ratio of the training instance\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],          # Subsample ratio of columns when constructing each tree\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],               # L1 regularization term on weights\n",
    "    'reg_lambda': [0.01, 0.1, 1, 10],             # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=model_xgb,\n",
    "                                   param_distributions=param_grid,\n",
    "                                   n_iter=100,           # Number of different combinations to try\n",
    "                                   scoring='neg_mean_squared_error',  # Use a suitable scoring method\n",
    "                                   cv=3,                 # Cross-validation\n",
    "                                   verbose=2,            # To see the progress\n",
    "                                   n_jobs=-1,            # Use all processors\n",
    "                                   random_state=42)      # Ensure reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best parameters found: {'subsample': 1.0, 'reg_lambda': 10, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0.5, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor:\n",
      "MSE: 373865999363.78064\n",
      "RMSE: 611445.8270065964\n",
      "R2: 0.8747429251670837\n"
     ]
    }
   ],
   "source": [
    "model_xgb = XGBRegressor(subsample=1.0,reg_lambda =10,reg_alpha=0,n_estimators=500,min_child_weight=5,max_depth = 5,learning_rate= 0.2,gamma=0.5, colsample_bytree= 0.8)\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBRegressor:\")\n",
    "print(\"MSE:\", mse_xgb)\n",
    "print(\"RMSE:\", rmse_xgb)\n",
    "print(\"R2:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor:\n",
      "MSE: 188392201549.15237\n",
      "RMSE: 434041.7048500666\n",
      "R2: 0.9368826150894165\n"
     ]
    }
   ],
   "source": [
    "model_xgb = XGBRegressor()\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBRegressor : \")\n",
    "print(\"MSE:\", mse_xgb)\n",
    "print(\"RMSE:\", rmse_xgb)\n",
    "print(\"R2:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingRegressor:\n",
      "MSE: 661697067930.2336\n",
      "RMSE: 813447.6430171973\n",
      "R2: 0.7783103633757368\n"
     ]
    }
   ],
   "source": [
    "# Model 2: HistGradientBoostingRegressor\n",
    "hgb_model = HistGradientBoostingRegressor()\n",
    "hgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_hgb = hgb_model.predict(X_test_scaled)\n",
    "mse_hgb = mean_squared_error(y_test, y_pred_hgb)\n",
    "rmse_hgb = np.sqrt(mse_hgb)\n",
    "r2_hgb = r2_score(y_test, y_pred_hgb)\n",
    "\n",
    "print(\"HistGradientBoostingRegressor:\")\n",
    "print(\"MSE:\", mse_hgb)\n",
    "print(\"RMSE:\", rmse_hgb)\n",
    "print(\"R2:\", r2_hgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor:\n",
      "MSE: 252286232049.02133\n",
      "RMSE: 502281.02895592357\n",
      "R2: 0.915476060241287\n"
     ]
    }
   ],
   "source": [
    "# Model 4: RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=500)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"RandomForestRegressor:\")\n",
    "print(\"MSE:\", mse_rf)\n",
    "print(\"RMSE:\", rmse_rf)\n",
    "print(\"R2:\", r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gr_model = GradientBoostingRegressor(n_estimators=500)\n",
    "Gr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_rf = Gr_model.predict(X_test_scaled)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"RandomForestRegressor:\")\n",
    "print(\"MSE:\", mse_rf)\n",
    "print(\"RMSE:\", rmse_rf)\n",
    "print(\"R2:\", r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models\\\\model_xgb.pkl', 'wb') as file:\n",
    "    pickle.dump(model_xgb, file)\n",
    "with open('Models\\\\scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "with open('Models\\\\onehot_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(onehot_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = pd.read_csv('Dataset\\\\X_train_1.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mileage',\n",
       " 'Torque',\n",
       " 'Length',\n",
       " 'Width',\n",
       " 'Height',\n",
       " 'Wheel Base',\n",
       " 'Kerb Weight',\n",
       " 'Max Power']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
